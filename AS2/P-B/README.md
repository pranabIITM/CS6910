## Part B - Finetuning a pre-trained CNN model
<hr>
#### Goal - Learn transfer learning and fine-tuning a pre-trained CNN model. <br>
<br>
### Dataset used - iNaturalist-12K <br>
<br>
### Libraries used -
1. tensorflow
2. keras
3. sys
4. math
5. numpy and matplotlib - To plot sample images (optional)
6. wandb - To generate sweeps (optional)
<br>

### Available options -
#### 1. Pre-trained model - 
        InceptionV3
        InceptionResNetV2
        ResNet50
        Xception
        
#### 2. Optimizers - 
        rmsprop
        adam
        adamax
<br>
### Source code file - <br>
1. PartB.ipynb - To run code using wandb <br>
2. PartBTerminal.py - To run code in terminal without wandb <br>
<br>
### Instructions to run PartB.ipynb - <br>
(suggestion - use google colab for coding)
1. Load dataset. <br>
   Follow the below code if you are using google colab and the "inaturalist_12K" data folder is uploaded on your google drive.<br>
```
from google.colab import drive
drive.mount('/content/gdrive')

train_path = '/content/gdrive/My Drive/inaturalist_12K/train'
val_path = '/content/gdrive/My Drive/inaturalist_12K/val'
```
<br>
or change the code accordingly to load the dataset and set the train and validation folders path. <br>
<br>

2. Install dependencies <br>
```
!pip install wandb
```
<br>
3. Run all the code blocks of the "PartB.ipynb" notebook.<br>
4. click in the sweep link generated by wandb.agent to track the graphs of different parameters given in sweep config.<br>
sweep configuration used here -
```
sweep_config = {
    'method' : 'random', 
    'metric' : {
        'name': 'val_accuracy',
        'goal': 'maximize'   
        },
    'parameters' : {
        'epochs': {'values' : [5,10]},
        'model' : {'values' : ['InceptionV3','InceptionResNetV2','ResNet50','Xception']},
        'optimizer' : {'values' : ['rmsprop','adam', 'adamax']},
        'batch_size' : {'values' : [16,32,64]},
        'lr' : {'values' : [1e-4,1e-5]},
        'data_aug' : {'values' : [True,False]},
        'freeze' : {'values' : [0.6,0.7,0.8,0.9]}
    }
}
```
<br>
### Instructions to run PartBTerminal.py - <br>
1. Move the "PartBTerminal.py" to the same directory where the "inaturalist_12k" folder is saved<br>
   or change the code to set the train and validation folders path.<br>
```
train_path = 'inaturalist_12K/train'
val_path = 'inaturalist_12K/val'
```
<br>
2. open the terminal and go to directory where the code file is and run the code with the following command line arguments - 
argument 1 - Number of epochs
argument 2 - Pre-trained model name (Choose from the "Available options")
argument 3 - Optimizer (Choose from the "Available options")
argument 4 - Batch size
argument 5 - Learning rate
argument 6 - Whether to do data augmentation (True/False)
argument 7 - Fraction of total layers to be freezed in the pre-trained model during fine-tuning 
Example - 
```
python gdrive/MyDrive/partb.py 5 InceptionV3 adamax 32 0.0001 False 0.9
```
<br>
3. Output will show loss and accuracy of the model on train, validation and test set generated by keras.
Example - 
```
Epoch 5/5
281/281 [==============================] - 188s 669ms/step - loss: 0.2795 - accuracy: 0.9237 - val_loss: 0.6141 - val_accuracy: 0.8024 - _timestamp: 1648841975.0000 - _runtime: 2082.0000
63/63 [==============================] - 562s 9s/step - loss: 0.6121 - accuracy: 0.8170
```
