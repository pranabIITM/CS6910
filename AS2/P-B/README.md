## Part B - Finetuning a pre-trained CNN model
<hr>

#### Goal - Learn transfer learning and fine-tuning a pre-trained CNN model. 


### Dataset used - iNaturalist-12K 


### Libraries used - 

1. tensorflow 

2. keras 

3. sys

4. math 

5. numpy and matplotlib - To plot sample images (optional) 

6. wandb - To generate sweeps (optional)



### Available options -
#### 1. Pre-trained model - 
        InceptionV3
        InceptionResNetV2
        ResNet50
        Xception
        
#### 2. Optimizers - 
        rmsprop
        adam
        adamax
<br>
### Source code file - <br>
1. PartB.ipynb - To run code using wandb <br>
2. PartBTerminal.py - To run code in terminal without wandb <br>
<br>
### Instructions to run PartB.ipynb - 

(suggestion - use google colab for coding)

1. Load dataset.

Follow the below code if you are using google colab and the "inaturalist_12K" data folder is uploaded on your google drive. <br>
```
from google.colab import drive
drive.mount('/content/gdrive')
train_path = '/content/gdrive/My Drive/inaturalist_12K/train'
val_path = '/content/gdrive/My Drive/inaturalist_12K/val'
```

or change the code accordingly to load the dataset and set the train and validation folders path. <br>


2. Install dependencies <br>
```
!pip install wandb
```

3. Run all the code blocks of the "PartB.ipynb" notebook.

4. click in the sweep link generated by wandb.agent to track the graphs of different parameters given in sweep config.

sweep configuration used here -
```
sweep_config = {
    'method' : 'random', 
    'metric' : {
        'name': 'val_accuracy',
        'goal': 'maximize'   
        },
    'parameters' : {
        'epochs': {'values' : [5,10]},
        'model' : {'values' : ['InceptionV3','InceptionResNetV2','ResNet50','Xception']},
        'optimizer' : {'values' : ['rmsprop','adam', 'adamax']},
        'batch_size' : {'values' : [16,32,64]},
        'lr' : {'values' : [1e-4,1e-5]},
        'data_aug' : {'values' : [True,False]},
        'freeze' : {'values' : [0.6,0.7,0.8,0.9]}
    }
}
```

### Instructions to run PartBTerminal.py -
1. Move the "PartBTerminal.py" to the same directory where the "inaturalist_12k" folder is saved<br>
   or change the code to set the train and validation folders path.
```
train_path = 'inaturalist_12K/train'
val_path = 'inaturalist_12K/val'
```

2. open the terminal and go to directory where the code file is and run the code with the following command line arguments - 

argument 1 - Number of epochs

argument 2 - Pre-trained model name (Choose from the "Available options")

argument 3 - Optimizer (Choose from the "Available options")

argument 4 - Batch size

argument 5 - Learning rate

argument 6 - Whether to do data augmentation (True/False)

argument 7 - Fraction of total layers to be freezed in the pre-trained model during fine-tuning 

Example - 
```
python gdrive/MyDrive/partb.py 5 InceptionV3 adamax 32 0.0001 False 0.9
```

3. Output will show loss and accuracy of the model on train, validation and test set generated by keras.

Example - 
```
Epoch 5/5
281/281 [==============================] - 188s 669ms/step - loss: 0.2795 - accuracy: 0.9237 - val_loss: 0.6141 - val_accuracy: 0.8024 - _timestamp: 1648841975.0000 - _runtime: 2082.0000
63/63 [==============================] - 562s 9s/step - loss: 0.6121 - accuracy: 0.8170
```
